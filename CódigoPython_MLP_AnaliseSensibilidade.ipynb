{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Organização prévia\n",
    "\n",
    "## 1.1 Módulos que serão utilizados\n",
    "## 1.2 Funções implementadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT MODULES\n",
    "import os\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt \n",
    "import operator\n",
    "import xlwt\n",
    "from xlwt.Workbook import *\n",
    "from pandas import ExcelWriter\n",
    "import xlsxwriter\n",
    "import math as m\n",
    "import matplotlib.cm as cm\n",
    "from pandas.plotting import scatter_matrix\n",
    "import itertools\n",
    "from matplotlib import patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function 1: Obtaining the DataBase\n",
    "def database(path, file):\n",
    "    #Change directory\n",
    "    os.chdir(path)\n",
    "    #Data set\n",
    "    db=pd.read_csv(file,sep=\",\",encoding=\"ISO-8859-1\")\n",
    "    #Data Matrix shape\n",
    "    [l,c]=db.shape\n",
    "    #Columns Header (Variables)\n",
    "    header=db.dtypes.index\n",
    "    return (db,l,c,header)\n",
    "\n",
    "#Function 2.0: Three-Sigma Rule\n",
    "def nsigma(var_orig, n=3):\n",
    "    '''n -> multiplication factor of the threshold'''\n",
    "    var=var_orig.copy()\n",
    "    mean=var.mean(0)\n",
    "    std=var.std(0)\n",
    "    difference=abs(var-mean)\n",
    "    threshold = n*std\n",
    "    outlier_idx=difference>threshold\n",
    "    var[outlier_idx]=np.nan\n",
    "    return (var,outlier_idx,threshold)\n",
    "\n",
    "#Function 2.1: Hampel Filter - moving window\n",
    "def hampel(var_orig, rol='y', k=7, t0=3):\n",
    "    '''vals: pandas series of values from which to remove outliers\n",
    "    k: size of window (including the sample; 7 is equal to 3 on either side of value)'''\n",
    "    #Make copy so original not edited\n",
    "    var=var_orig.copy()    \n",
    "    #Hampel Filter\n",
    "    L= 1.4826\n",
    "    if rol=='y':\n",
    "        rolling_median=var.rolling(k).median()#Median calculatade for each window, correspondent to each row -> the first (k-1) lines are set to NaN\n",
    "        difference=np.abs(rolling_median-var)# The k first are valued as NaN\n",
    "        median_abs_deviation=difference.rolling(k).median()\n",
    "    else:\n",
    "        median=var.median()#original Hampel\n",
    "        difference=np.abs(median-var)\n",
    "        median_abs_deviation= difference.median()\n",
    "    threshold= t0 *L * median_abs_deviation\n",
    "    outlier_idx=difference>threshold # NaN compered to anything always returns False -> the first k-1 values are never considered outlier\n",
    "    var[outlier_idx]=np.nan\n",
    "    return(var,outlier_idx,threshold)\n",
    "\n",
    "#Function 3: Data Normalization\n",
    "def scale(train,test,sk=1,rang=(0,1)):\n",
    "    #sk=1 -> MinMaxScaler\n",
    "    #sk=2 -> StandardScaler\n",
    "    if sk==1:\n",
    "        scaler=MinMaxScaler(feature_range=rang)\n",
    "    else:\n",
    "        scaler=StandardScaler()\n",
    "    if len(train.shape)==1:\n",
    "        tr=train.values.reshape(-1,1)\n",
    "        te=test.values.reshape(-1,1)\n",
    "        tr=np.reshape(scaler.fit_transform(tr),len(train))\n",
    "        te=np.reshape(scaler.transform(te),len(test))\n",
    "    else:\n",
    "        tr=scaler.fit_transform(train)\n",
    "        te=scaler.transform(test)\n",
    "    return (tr,te,scaler)\n",
    "\n",
    "#Function 4: Separate Database into input variables and output variables\n",
    "def inout(data,header,output_number=1):\n",
    "    #Output variables\n",
    "    yname=header[-output_number]\n",
    "    y=data[yname]\n",
    "    #Input variables\n",
    "    xname=header[:-output_number]\n",
    "    x=data[xname]\n",
    "    xs=x.shape\n",
    "    ys=y.shape\n",
    "    return(x,xname,xs,y,yname,ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Leitura da base de dados\n",
    "\n",
    "## 2.1 Base de dados por completo\n",
    "## 2.2 Base de dados apenas com as variáveis que serão efetivamente utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBTAIN THE DATABASE\n",
    "path=\"C:/Users/anab-/Documents/EQ Mestrado/A Pesquisa/Dados e Resultados/Caldeira de Recuperação\"\n",
    "file=\"db_20vc.csv\"\n",
    "db_orig,l,c,header_orig=database(path,file)\n",
    "#The first and second columns are index and month -> should be removed\n",
    "header=header_orig[2::]\n",
    "dbb=db_orig.loc[:,header]\n",
    "#Descriptive statistics\n",
    "stat=dbb.describe()\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CREATE A DIRECTORY TO SEND THE RESULTS\n",
    "db=dbb.drop(['16-P_BD_KG/CM2'],axis=1)\n",
    "dir = 'Results_new_outPBD'\n",
    "if not os.path.isdir(dir): os.makedirs(dir)\n",
    "\n",
    "figsize=(12,8)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "db.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNIVARIATE SCATTER PLOTS\n",
    "for i in range(db.shape[1]):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(db.iloc[:,[i]],'kx',alpha=0.5)\n",
    "    plt.xlabel('Observation')\n",
    "    plt.ylabel(db.columns[i])\n",
    "    plt.title(db.columns[i])\n",
    "    fname = 'BeforeFilter-' + str(i+1) + '.png'\n",
    "    plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXCLUDE NOT USED VARIABLES\n",
    "db0=db.drop(['17-C_H2S_PPM', '19-F_STREAM_T/H', '20-E_REDUC_%'],axis=1).replace(0,np.nan).dropna()\n",
    "header0=db0.dtypes.index\n",
    "\n",
    "#Data base after deletion of the SO2 disparate data\n",
    "db01=db0.copy() #Unly the input variable included in the model\n",
    "db01=db01.iloc[0:2395].append(db01.iloc[2545::])\n",
    "\n",
    "db01.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNIVARIATE SCATTER PLOTS\n",
    "'''plt.figure(figsize=(12,8))\n",
    "plt.plot(db0.iloc[:,[-1]],'kx',alpha=0.5)\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel(header0[-1])\n",
    "plt.title(header0[-1])\n",
    "fname = 'SO2beforeremoval-' + str(i+1) + '.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)\n",
    "for i in range(db01.shape[1]):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(db01.iloc[:,[i]],'kx',alpha=0.5)\n",
    "    if i==16:\n",
    "        plt.ylim((0,600))\n",
    "    plt.xlabel('Observation')\n",
    "    plt.ylabel(header0[i])\n",
    "    plt.title(header0[i])\n",
    "    \n",
    "fname = 'SO2afterremoval-' + str(i+1) + '.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)\n",
    "'''\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(db0.iloc[2395:2545,[-1]],'kx',alpha=1,label='Dados Removidos')\n",
    "plt.plot(db01.iloc[:,[-1]],'k.',alpha=0.1,label='Dados Mantidos')\n",
    "plt.xlabel('Observação')#('Observation')\n",
    "plt.ylabel('SO2 (ppm)')#(header0[-1])\n",
    "plt.legend()\n",
    "plt.title('SO2 (ppm) - antes e depois da remoção do conjunto discrepante')#(header0[-1])\n",
    "fname = 'SO2beforeAfter-removal.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(db0.iloc[:,[-1]],'k-',alpha=1)\n",
    "#plt.plot(db01.iloc[:,[-1]],'k.',alpha=0.1,label='Dados Mantidos')\n",
    "plt.xlabel('Observação')#('Observation')\n",
    "plt.ylabel('SO2 (ppm)')#(header0[-1])\n",
    "plt.title('SO2 (ppm) - Dados completos')#(header0[-1])\n",
    "fname = 'SO2complete.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCATTER MATRIX  \n",
    "fig=scatter_matrix(db01, alpha=0.2, figsize=(20, 20), diagonal='hist')\n",
    "fname = 'Scatter Matrix - Original data'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)\n",
    "#Histograms\n",
    "plt.figure()\n",
    "db01.hist(grid=False,figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CORRELATION MATRIX\n",
    "cor=db0.corr() #Correlation Matrix\n",
    "cor.style.background_gradient(cmap='PuBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. PRÉ PROCESSAMENTO\n",
    "\n",
    "## 3.1 Cálculo das médias das variáveis e verificação da coerência\n",
    "## 3.2 Detecção de outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3.1 SUBSTITUTION OF '3-P_FUEL-MMH20' and '4-P_FUEL-MMH20' BY THE AVERAGE\n",
    "pfuel=db01.loc[:,('3-P_FUEL-MMH20','4-P_FUEL-MMH20')].copy()\n",
    "#Mean of the values\n",
    "meanfuel=(pfuel['3-P_FUEL-MMH20']+pfuel['4-P_FUEL-MMH20'])/2\n",
    "pfuel['3.5-P_FUELmean-MMH2O']=meanfuel\n",
    "\n",
    "#SCATTER MATRIX\n",
    "scatter_matrix(pfuel, alpha=0.2, figsize=(8,8), diagonal='hist')\n",
    "fname = 'Scatter-matrix p fuel.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)\n",
    "#Correlation Matrix\n",
    "corfuel=pfuel.corr() \n",
    "corfuel.style.background_gradient(cmap='PuBu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 SUBSTITUTION OF '5-SS1_%' and '6-SS2_%' BY THE AVERAGE\n",
    "ssolid=db01.loc[:,('5-SS1_%', '6-SS2_%')].copy()\n",
    "#Mean of the values\n",
    "meansolid=(ssolid['5-SS1_%']+ssolid['6-SS2_%'])/2\n",
    "ssolid['5.5-SSmean_%']=meansolid\n",
    "\n",
    "#SCATTER MATRIX\n",
    "scatter_matrix(ssolid, alpha=0.2, figsize=(8, 8), diagonal='hist')\n",
    "fname = 'Scatter-matrix solids.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)\n",
    "#Correlation Matrix\n",
    "corsolid=ssolid.corr() \n",
    "corsolid.style.background_gradient(cmap='PuBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.1 SUBSTITUTION OF THE TWO PRESSURES AND TWO SOLIDS CONTENTS MEASUREMENTS BY THEIR MEANS\n",
    "db1 = db01.copy()\n",
    "# Unification of '3-P_FUEL-MMH20' and '4-P_FUEL-MMH20' - Mean of pressures\n",
    "Fm=(db1['3-P_FUEL-MMH20']+db1['4-P_FUEL-MMH20'])/2\n",
    "#Modification of the database\n",
    "db1['3-P_FUEL-MMH20']=Fm\n",
    "db1.drop(['4-P_FUEL-MMH20'], axis=1, inplace=True)# Removing columns\n",
    "db1.rename(columns={'3-P_FUEL-MMH20': '3.5-P_FUELmean-MMH2O'}, inplace=True)\n",
    "\n",
    "# Unification of '5-SS1_%' and '6-SS2_%' - Mean Solids Content\n",
    "SSm=(db1['5-SS1_%']+db1['6-SS2_%'])/2\n",
    "#Modification of the database\n",
    "db1['5-SS1_%']=SSm #Adding the %SS mean values to the new database\n",
    "db1.drop(['6-SS2_%'], axis=1, inplace=True)# Removing columns\n",
    "db1.rename(columns={'5-SS1_%': '5.5-SSmean_%'}, inplace=True)\n",
    "\n",
    "head1= db1.dtypes.index\n",
    "db1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3.1 CORRELATION MATRIX\n",
    "cor1=db1.corr() #Correlation Matrix\n",
    "cor1.style.background_gradient(cmap='PuBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3.2 OUTLIER DETECTION\n",
    "workfilter=db1.copy()\n",
    "#Comparision of different filters\n",
    "    #Those functions return the work database with Nan in place of Outliers and the index of the outliers\n",
    "db_f1,out_idxf1,threshold1 = nsigma(workfilter,n=3) #Mean +- 3 sigma\n",
    "workfilter=db1.copy()\n",
    "db_f2,out_idxf2,threshold2 = hampel(workfilter,rol='n') #Just 1.4826*3*MAD\n",
    "workfilter=db1.copy()\n",
    "db_f3,out_idxf3,threshold3 = hampel(workfilter,rol='y') #Moving window\n",
    "db_f1=db_f1.dropna()\n",
    "db_f2=db_f2.dropna()\n",
    "db_f3=db_f3.dropna()\n",
    "print(np.sum(out_idxf1),db_f1.shape, np.sum(out_idxf2),db_f2.shape, np.sum(out_idxf3),db_f3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 CHART OUTLIER DETECTION\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(db1.iloc[:,-1],'k.',label='SO2 values')\n",
    "plt.plot((0,db1.index.max()),(db1.iloc[:,-1].mean()-threshold1.iloc[-1],db1.iloc[:,-1].mean()-threshold1.iloc[-1]),'k--',label='3sigma threshold')\n",
    "plt.plot((0,db1.index.max()),(db1.iloc[:,-1].mean()+threshold1.iloc[-1],db1.iloc[:,-1].mean()+threshold1.iloc[-1]),'k--')\n",
    "plt.plot((0,db1.index.max()),(db1.iloc[:,-1].mean(),db1.iloc[:,-1].mean()),'k-',label='Mean')\n",
    "plt.plot((0,db1.index.max()),(db1.iloc[:,-1].median()-threshold2.iloc[-1],db1.iloc[:,-1].median()-threshold2.iloc[-1]),'r:',label='Hampel threshold')\n",
    "plt.plot((0,db1.index.max()),(db1.iloc[:,-1].median()+threshold2.iloc[-1],db1.iloc[:,-1].median()+threshold2.iloc[-1]),'r:')\n",
    "plt.plot((0,db1.index.max()),(db1.iloc[:,-1].median(),db1.iloc[:,-1].median()),'r-',label='Median')\n",
    "plt.legend()\n",
    "#plt.ylim((0,200))#There is a outlier out there to change the mean\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('SO2 (ppm)')\n",
    "plt.title('Comparison 3sigma and Hampel-SO2')\n",
    "\n",
    "fname = 'Comparison 3sigma and Hampel-SO2.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3.2 Chosen Filter -> Hampel identifier\n",
    "work=db_f2.copy()\n",
    "head=work.dtypes.index\n",
    "db_f2.describe()\n",
    "threshold2#.iloc[-1]\n",
    "2860-2108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3.2 SCATTER MATRIX\n",
    "scatter_matrix(work, alpha=0.2, figsize=(16, 16), diagonal='hist') \n",
    "plt.savefig(os.path.join(dir, 'Scatter-db-f2.png'), bbox_inches='tight', format='png', dpi=600)\n",
    "#CORRELATION MATRIX\n",
    "corw=work.corr() #Correlation Matrix\n",
    "corw.style.background_gradient(cmap='PuBu')#'coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Univariate plots\n",
    "for i in range(db1.shape[1]):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(db1.iloc[:,i],'k--',alpha=0.5,label='Dados antes filtro')#Data before filtering\n",
    "    plt.plot(work.iloc[:,i],'k-',alpha=1, label='Dados após filtro') #Data after filtering\n",
    "    plt.legend()\n",
    "    if i==db1.shape[1]-1:\n",
    "        plt.ylim((25,250))\n",
    "        plt.legend(loc='upper left')\n",
    "    plt.xlabel('Observation')\n",
    "    plt.ylabel(head1[i])\n",
    "    plt.title(head1[i])\n",
    "    fname = 'AfterFilter-' + str(i+1) + '.png'\n",
    "    plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Validação cruzada e Normalização\n",
    "## 3.1 Validação cruzada\n",
    "## 3.2 Normalização\n",
    "## 3.3 Verificação dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Separation of the rows with the highest and te lowest values of each variable, in order to ensure that they will be assigned to training set\n",
    "kn=5\n",
    "rs=42\n",
    "\n",
    "#DATABASE SELECTED: \n",
    "data=work.copy()\n",
    "head=work.dtypes.index\n",
    "#Separation of the data: Input and Output\n",
    "x,xname,xs,y,yname,ys=inout(data,head,output_number=1) #xs,ys = shapes of x and y\n",
    "\n",
    "ind=[]\n",
    "for i in range(x.shape[1]):\n",
    "    ind.append(x[xname[i]].idxmax())\n",
    "    ind.append(x[xname[i]].idxmin())\n",
    "ind.append(y.idxmax())\n",
    "ind.append(y.idxmin())\n",
    "xminmax=x.loc[set(ind)]\n",
    "xrest=x.drop(set(ind))\n",
    "yminmax=y.loc[set(ind)]\n",
    "yrest=y.drop(set(ind))\n",
    "\n",
    "#Separation of the data into training set and testing set\n",
    "ts= 0.2535#0.2005 #Testar 0.2535\n",
    "xtr, xte, ytr, yte=train_test_split(xrest, yrest, test_size=ts, random_state=rs) \n",
    "xtr=xtr.append(xminmax)\n",
    "ytr=ytr.append(yminmax)\n",
    "xtr.shape,xtr.shape[0]/5, xte.shape[0]/data.shape[0], xte.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Normalização\n",
    "#MinMaxScaler\n",
    "        #For tanh: x and y => [-1,+1]\n",
    "        #For logistic: x=>[-1,+1]; y=>[0,+1]\n",
    "xtrain,xtest,scalerx=scale(xtr,xte,sk=1,rang=(-1,1))\n",
    "ytrain,ytest,scalery=scale(ytr,yte,sk=1,rang=(-1,1))\n",
    "ytrainl,ytestl,scaleryl=scale(ytr,yte,sk=1,rang=(0,1))\n",
    "    #StandardScaler\n",
    "xtrain1,xtest1,scalerx1=scale(xtr,xte,sk=2)\n",
    "ytrain1,ytest1,scalery1=scale(ytr,yte,sk=2) #StandardScaler does not need a range definition\n",
    "ytrai=ytrain.copy()\n",
    "ytes=ytest.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Boxplot\n",
    "medianprops = dict(linestyle=':', linewidth=2, color='k')\n",
    "for j in range(x.shape[1]):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.boxplot([xtrain[:,j],xtest[:,j]],labels=['xtrain','xtest'], medianprops=medianprops)\n",
    "    plt.title(head[j])\n",
    "    fname = 'Boxplot-' + str(j+1) + '.png'\n",
    "    plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.boxplot([ytrain,ytest],labels=['ytrain','ytest'], medianprops=medianprops)\n",
    "plt.title(head[-1])\n",
    "fname = 'Boxplot-' + str(15) + '.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modelos de Regressão\n",
    "## 4.1 Regressão Linear Múltipla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4.1 Results from Multiple linear regression\n",
    "\n",
    "n=0\n",
    "kn=5 #number of splits at cross-validation procedure\n",
    "\n",
    "#Writing the answer\n",
    "column=['Y test','ŷ-cv1','ŷ-cv2', 'ŷ-cv3','ŷ-cv4','ŷ-cv5','ŷ-all','r2-cv1','r2-cv2','r2-cv3','r2-cv4','r2-cv5','r2-cvmean','r2-all']\n",
    "mlr=pd.DataFrame(index=yte.index,columns=column).fillna(0)\n",
    "mlr.iloc[:,0]=yte #Y value from database\n",
    "\n",
    "# MULTIPLE LINEAR REGRESSION \n",
    "reg=sk.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)\n",
    "R2=np.zeros([1,kn])\n",
    "#Cross-Validation\n",
    "kf=sk.model_selection.KFold(n_splits=kn, shuffle=True, random_state=rs)\n",
    "    #The training data is used for cross validation procedure. Te first testg set separeted is used after, for testing (https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6)\n",
    "    #Data set for cross validation: xtrain, ytrain\n",
    "i=0\n",
    "for train_index, test_index in kf.split(xtrain,ytrain):\n",
    "    x_train, x_test = xtrain[train_index], xtrain[test_index]\n",
    "    y_train, y_test = ytrain[train_index], ytrain[test_index]\n",
    "\n",
    "    #Treining\n",
    "    freg=reg.fit(x_train,y_train)\n",
    "    parr=freg.get_params(deep=True)\n",
    "    #Prediction\n",
    "    y_hat=freg.predict(x_test) #Cross-validation prediction\n",
    "    yhat=freg.predict(xtest) #Test Prediction\n",
    "\n",
    "    #Return to original scale\n",
    "    Y_hat=scalery.inverse_transform(y_hat.reshape(-1,1))\n",
    "    Yhat=scalery.inverse_transform(yhat.reshape(-1,1))\n",
    "\n",
    "    #Coefficient of Determination\n",
    "    r2=freg.score(x_test,y_test) # Cross validation test set\n",
    "    R2test=freg.score(xtest,ytest) #Real test set\n",
    "    mlr.iloc[:,i+1]=Yhat #Crossvalidation\n",
    "    mlr.iloc[:,i+7]=r2#R2test\n",
    "    R2[0,i]=r2\n",
    "    i+=1\n",
    "#Treinar o modelo com todos os dados\n",
    "freg=reg.fit(xtrain,ytrain)\n",
    "parr=freg.get_params(deep=True)\n",
    "#Prediction\n",
    "yhat=freg.predict(xtest) #Test Prediction\n",
    "Yhat=scalery.inverse_transform(yhat.reshape(-1,1)) #Return to original scale\n",
    "R2test=freg.score(xtest,ytest) #Real test set\n",
    "mlr.iloc[:,6]=Yhat\n",
    "mlr.iloc[:,-2]=mlr.iloc[:,[7,8,9,10,11]].mean(axis=1)\n",
    "mlr.iloc[:,-1]=R2test\n",
    "\n",
    "stdev=mlr.iloc[:,[7,8,9,10,11]].std(axis=1).iloc[0]\n",
    "\n",
    "mlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4.1 Results of MLR\n",
    "import math as m\n",
    "print(\" r2_cv mean: \",mlr.iloc[0,-2],\"\\n\",\"r2_complete data: \", mlr.iloc[0,-1])\n",
    "print(\" r_cv mean: \",m.sqrt(mlr.iloc[0,-2]),\"\\n\",\"r_complete data: \", m.sqrt(mlr.iloc[0,-1]))\n",
    "print(\" residuals mean and stdev: \", (mlr.iloc[:,0]-mlr.iloc[:,6]).mean(0),(mlr.iloc[:,0]-mlr.iloc[:,6]).std(0))\n",
    "print(\" Maximum and Minimum error: \", abs(mlr.iloc[:,0]-mlr.iloc[:,6]).max(),abs(mlr.iloc[:,0]-mlr.iloc[:,6]).min())\n",
    "print(\" Mean absolute error: \",sum(abs(mlr.iloc[:,0]-mlr.iloc[:,6]))/mlr.iloc[:,6].shape[0])\n",
    "\n",
    "#Coeficientes da regressão linear\n",
    "print(freg.coef_ , freg.intercept_)\n",
    "print(parr)\n",
    "# 4.1 Plot results from multiple linear regression\n",
    "plt.figure(3,figsize=(8,8))\n",
    "plt.plot([75,115],[75,115],'-.', color='grey')\n",
    "plt.plot(mlr.iloc[:,0],mlr.iloc[:,6],'k.') #r2 cv mean\n",
    "plt.text(76,112,'R2 test: '+str(round(mlr.iloc[0,-1],4))+'\\n'+'R test: '+str(round(np.sqrt(mlr.iloc[0,-1]),4)), fontsize=12)\n",
    "\n",
    "plt.title('Multiple Linear Regression Avaliation: ŷ vs y')\n",
    "plt.ylim((75,115))\n",
    "plt.ylabel('ŷ (SO2 estimatives)')\n",
    "plt.xlim((75,115))\n",
    "plt.xlabel('ytest (SO2 real values)')\n",
    "fname = 'MLR_ŷ-y' + '.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)\n",
    "\n",
    "plt.figure(4,figsize=(8,8))\n",
    "plt.hist(sorted(mlr.iloc[:,0]-mlr.iloc[:,6]),50, density=True, color='gray')\n",
    "plt.xlabel('y-ŷ')\n",
    "plt.ylabel('Probability density')\n",
    "plt.title('Histogram of residuals - multiple linear regression'+'\\n'+'Absolute Values')\n",
    "plt.text(-16,0.085,'Residuals mean: '+str(round((mlr.iloc[:,0]-mlr.iloc[:,6]).mean(0),4))+'\\n'+'Residuals standard deviation: '+str(round((mlr.iloc[:,0]-mlr.iloc[:,6]).std(0),4)), fontsize=10)\n",
    "fname = 'MLR-Histogram of residuals-absolute' + '.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)\n",
    "\n",
    "plt.figure(5,figsize=(8,8))\n",
    "plt.plot([0,2600],[0,0],'-.', color='grey')\n",
    "plt.plot((mlr.iloc[:,0]-mlr.iloc[:,6]),'k.')\n",
    "plt.ylabel('y-ŷ')\n",
    "plt.xlabel('Observation')\n",
    "plt.title('Dispersion of the residuals - multiple linear regression'+'\\n'+'Absolute Values')\n",
    "fname = 'MLR-Dispersion of the residuals MLR-absolute' + '.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)\n",
    "\n",
    "\n",
    "plt.figure(6,figsize=(8,8))\n",
    "plt.hist(sorted((mlr.iloc[:,0]-mlr.iloc[:,6])/mlr.iloc[:,0]),50, density=True, color='gray')\n",
    "plt.xlabel('(y-ŷ)/y')\n",
    "plt.ylabel('Probability density')\n",
    "plt.title('Histogram of residuals - multiple linear regression'+'\\n'+'Relative Values')\n",
    "plt.text(-0.16,8.5,'Residuals mean: '+str(round(((mlr.iloc[:,0]-mlr.iloc[:,6])/mlr.iloc[:,0]).mean(0),4))+'\\n'+'Residuals standard deviation: '+str(round(((mlr.iloc[:,0]-mlr.iloc[:,6])/mlr.iloc[:,0]).std(0),4)), fontsize=10)\n",
    "fname = 'MLR-Histogram of residuals-relative' + '.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)\n",
    "\n",
    "plt.figure(7,figsize=(8,8))\n",
    "plt.plot([0,2600],[0,0],'-.', color='grey')\n",
    "plt.plot((mlr.iloc[:,0]-mlr.iloc[:,6])/mlr.iloc[:,0],'k.')\n",
    "plt.ylabel('(y-ŷ)/y')\n",
    "plt.xlabel('Observation')\n",
    "plt.title('Dispersion of the residuals - multiple linear regression'+'\\n'+'Relative Values')\n",
    "fname = 'MLR-Dispersion of the residuals MLR-relative' + '.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE: Mean Squared Error\n",
    "mse_mlr=sum((mlr.iloc[:,0]-mlr.iloc[:,6])*(mlr.iloc[:,0]-mlr.iloc[:,6]))/mlr.iloc[:,0].shape[0]\n",
    "mse_mlr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Rede Perceptron em multicamadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.2 Multilayer Perceptron\n",
    "#Number of hidden layers variation\n",
    "N=30 #maximal number of neurons at hidden layer\n",
    "kn=5 #number of splits at cross-validation procedure\n",
    "t=1e-3 #Tolerance (modifications at r2 - > training the net)\n",
    "niter=7\n",
    "\n",
    "#Neural Network parameters\n",
    "n_neurons=list(range(1,N+1))\n",
    "activs=['logistic', 'tanh']\n",
    "solvs = ['lbfgs', 'sgd']\n",
    "learn_rate= ['constant', 'invscaling', 'adaptive']\n",
    "param_test=[] #MLP - Parameter names mapped to their values. For each test\n",
    "\n",
    "#Writing the answer\n",
    "cvn=ytr.shape[0]/5\n",
    "ind1=np.arange(cvn)\n",
    "ind2=np.array(yte.index)\n",
    "column1=['ŷ-cv1','ŷ-cv2', 'ŷ-cv3','ŷ-cv4','ŷ-cv5','r2-cv1','r2-cv2','r2-cv3','r2-cv4','r2-cv5','r2_mean','r2_stdev']\n",
    "column2=['Y_test','ŷ-test','r2-all','r2-cv1','r2-cv2','r2-cv3','r2-cv4','r2-cv5','r2_mean']\n",
    "column3=['r-cv1','r-cv2','r-cv3','r-cv4','r-cv5','r_mean','r_stdev']\n",
    "idx = pd.IndexSlice\n",
    "index1= pd.MultiIndex.from_product([n_neurons, activs, solvs, learn_rate,ind1], \n",
    "                          names=['ActivFunc','Solver','LearnRate','NumberNeurons','Observation'])\n",
    "index2= pd.MultiIndex.from_product([n_neurons, activs, solvs, learn_rate, ind2], \n",
    "                          names=['ActivFunc','Solver','LearnRate','NumberNeurons','Observation'])\n",
    "mlp_cv=pd.DataFrame(index=index1,columns=column1)#Results from crossvalidation procedure\n",
    "mlp_test=pd.DataFrame(index=index2,columns=column2)#Results all training set and test set (reidentificação da rede)\n",
    "r_coef=pd.DataFrame(index=index1,columns=column3)#Results Correlation coefficient(r), crossvalidation\n",
    "\n",
    "for m in range(N):#N #Neurons number\n",
    "    m=m+1\n",
    "    for h in range(2):#2 #activation\n",
    "        if activs[h]=='logistic':\n",
    "            ytrai=ytrainl.copy()\n",
    "            ytes=ytestl.copy()\n",
    "        else:\n",
    "            ytrai=ytrain.copy()\n",
    "            ytes=ytest.copy()\n",
    "\n",
    "        for j in range(2):#2 #solver\n",
    "            for k in range(3):#3 #learning rate\n",
    "                nn=MLPRegressor(hidden_layer_sizes=(m+1,),\n",
    "                           activation=activs[h], \n",
    "                           solver=solvs[j],\n",
    "                           batch_size='auto',\n",
    "                           learning_rate= learn_rate[k],\n",
    "                           max_iter=1000,\n",
    "                           random_state=rs,\n",
    "                           tol=t,\n",
    "                           n_iter_no_change=niter,\n",
    "                           verbose=False)\n",
    "                #https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "                kf=sk.model_selection.KFold(n_splits=kn, shuffle=True, random_state=rs)\n",
    "                #The training data is used for cross validation procedure. The first testset sorted is used after, for testing (https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6)\n",
    "                #Data set for cross validation: xtrain, ytrain\n",
    "                i=0\n",
    "                for train_index, test_index in kf.split(xtrain,ytrai):\n",
    "                    x_train, x_test = xtrain[train_index], xtrain[test_index]\n",
    "                    y_train, y_test = ytrai[train_index], ytrai[test_index]\n",
    "\n",
    "                    #Training\n",
    "                    rb=nn.fit(x_train,y_train)\n",
    "                    par=rb.get_params(deep=True)\n",
    "                    #Prediction\n",
    "                    y_hat=nn.predict(x_test)\n",
    "                    #Test with the extra testing set - For each cross validation\n",
    "                    yhat=nn.predict(xtest)\n",
    "                    #Reshaping\n",
    "                    y_h=y_hat.reshape(-1,1)\n",
    "                    yh=yhat.reshape(-1,1)\n",
    "                    if activs[h]=='logistic' or activs[h]=='relu':\n",
    "                        Y_hat=scaleryl.inverse_transform(y_h)\n",
    "                        Yhat=scaleryl.inverse_transform(yh)\n",
    "                        y_t=scaleryl.inverse_transform(y_test.reshape(-1,1))\n",
    "                    else:\n",
    "                        Y_hat=scalery.inverse_transform(y_h)\n",
    "                        Yhat=scalery.inverse_transform(yh)\n",
    "                        y_t=scalery.inverse_transform(y_test.reshape(-1,1))\n",
    "                    r2=rb.score(x_test,y_test)\n",
    "                    R2=rb.score(xtest,ytes)\n",
    "\n",
    "                    mlp_cv.loc[idx[m,activs[h], solvs[j],learn_rate[k],:],column1[i]]=Y_hat\n",
    "                    mlp_cv.loc[idx[m,activs[h], solvs[j],learn_rate[k],:],column1[i+5]]=r2\n",
    "                    r_coef.loc[idx[m,activs[h], solvs[j],learn_rate[k],:],column3[i]]=np.sqrt(r2)\n",
    "                    mlp_test.loc[idx[m,activs[h], solvs[j],learn_rate[k],:],column2[i+3]]=R2\n",
    "                    i+=1\n",
    "\n",
    "                #Training with all data\n",
    "                rb=nn.fit(xtrain,ytrai)\n",
    "                #Test with the extra testing set - For each cross validation\n",
    "                yhat=nn.predict(xtest)\n",
    "                yh=yhat.reshape(-1,1)\n",
    "                R2=rb.score(xtest,ytes)\n",
    "                if activs[h]=='logistic': \n",
    "                    Yhat=scaleryl.inverse_transform(yh)\n",
    "                else:\n",
    "                    Yhat=scalery.inverse_transform(yh)\n",
    "                #Writing the answer\n",
    "                mlp_test.loc[idx[m,activs[h], solvs[j],learn_rate[k],:],column2[0]]=yte.values.reshape(-1,1)\n",
    "                mlp_test.loc[idx[m,activs[h], solvs[j],learn_rate[k],:],column2[1]]=Yhat\n",
    "                mlp_test.loc[idx[m,activs[h], solvs[j],learn_rate[k],:],column2[2]]=R2\n",
    "                param_test.append([m,activs[h],solvs[j],learn_rate[k],nn.get_params()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4.2 MEAN OF R2 FROM CROSSVALIDATION (Determination coefficient)\n",
    "mlp_cv.loc[idx[:],column1[-2]]=mlp_cv.iloc[:,[5,6,7,8,9]].mean(axis=1)\n",
    "mlp_cv.loc[idx[:],column1[-1]]=mlp_cv.iloc[:,[5,6,7,8,9]].std(axis=1)\n",
    "mlp_test.loc[idx[:],column2[-1]]=mlp_test.iloc[:,[2,3,4,5,6]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4.2 MEAN OF R FROM CROSSVALIDATION (correlation coefficient)\n",
    "r_coef.loc[idx[:],column3[-2]]=r_coef.iloc[:,[0,1,2,3,4]].mean(axis=1)\n",
    "r_coef.loc[idx[:],column3[-1]]=r_coef.iloc[:,[0,1,2,3,4]].std(axis=1)\n",
    "r_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CROSS VALIDATION\n",
    "# 4.2 Take the better results per activation function for each neuron number at hidden layer\n",
    "best_log=np.zeros((N,6)).astype(object)\n",
    "best_tanh=np.zeros((N,6)).astype(object)\n",
    "for i in range(N):\n",
    "    best_log[i,:5]=r_coef.loc[idx[i+1,'logistic',:,:,:]].sort_values(['r_mean'], ascending=[False]).iloc[0].name\n",
    "    best_log[i,-2]=r_coef.loc[idx[i+1,'logistic',:,:,:]].sort_values(['r_mean'], ascending=[False]).iloc[0,-2] #r_mean\n",
    "    best_log[i,-1]=r_coef.loc[idx[i+1,'logistic',:,:,:]].sort_values(['r_mean'], ascending=[False]).iloc[0,-1] #r_stdev\n",
    "    best_tanh[i,:5]=r_coef.loc[idx[i+1,'tanh',:,:,:]].sort_values(['r_mean'], ascending=[False]).iloc[0].name\n",
    "    best_tanh[i,-2]=r_coef.loc[idx[i+1,'tanh',:,:,:]].sort_values(['r_mean'], ascending=[False]).iloc[0,-2] #r_mean\n",
    "    best_tanh[i,-1]=r_coef.loc[idx[i+1,'tanh',:,:,:]].sort_values(['r_mean'], ascending=[False]).iloc[0,-1] #r_stdev\n",
    "best_log, best_tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Plot best logistic results and best tanh results - cross validation\n",
    "xaxis=np.arange(1,31)\n",
    "plt.figure(1,figsize)\n",
    "plt.plot(xaxis,best_log[:,-2],'k.',label='logistic')\n",
    "plt.errorbar(xaxis, best_log[:,-2],best_log[:,-1],linewidth=0,elinewidth=0.5, color='grey')\n",
    "plt.plot(xaxis,best_tanh[:,-2],'kx',label='tanh')\n",
    "plt.errorbar(xaxis, best_tanh[:,-2],best_tanh[:,-1],linewidth=0,elinewidth=0.5, color='grey')\n",
    "plt.ylim(0.68,1.0)\n",
    "plt.xlabel('Neurons at hidden layer')\n",
    "plt.ylabel('r mean')\n",
    "plt.legend()\n",
    "plt.title('Cross validation corelation coefficient mean and errors for each activation function'+\"\\n\" +'according to the number of units at hidden layer')\n",
    "fname = 'r logistic and tanh' + '.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(N-1):\n",
    "    print(i+2, 100*(best_tanh[i+1,-2]-best_tanh[i,-2])/best_tanh[i,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.rcParams.update({'font.size': 16})\n",
    "#matplotlib.rc('xtick', labelsize=12) \n",
    "#matplotlib.rc('ytick', labelsize=12) \n",
    "# 4.2 Plot restrictly tanh, lbfgs, constant - Cross validation\n",
    "plt.figure(1,figsize)\n",
    "for i in range(1,N+1):\n",
    "    plt.errorbar(i,r_coef.loc[idx[i,'tanh','lbfgs','constant']].iloc[0,-2],yerr=r_coef.loc[idx[i,'tanh','lbfgs','constant']].iloc[0,-1],elinewidth=0.5, ecolor='black')#Mean of R2 of the crossvalidation regression\n",
    "    plt.plot(i,r_coef.loc[idx[i,'tanh','lbfgs','constant']].iloc[0,-2],'k.')\n",
    "plt.text(8.,0.938,9)\n",
    "plt.arrow(8.5,0.938,0.23,-0.007)\n",
    "plt.text(11.,0.94,12)\n",
    "plt.arrow(11.5,0.938,0.23,-0.007)\n",
    "\n",
    "plt.plot(xaxis,best_log[:,-2],'k.',label='logistic')\n",
    "plt.errorbar(xaxis, best_log[:,-2],best_log[:,-1],linewidth=0,elinewidth=0.5, color='grey')\n",
    "\n",
    "plt.title('Mean and standard deviation: r mean of cross-validation (actv function: tanh)')\n",
    "plt.ylabel('r mean')\n",
    "plt.xlabel('Number of neurons at hidden layer')\n",
    "fname = 'r mean of cross-validation x nn_logtanh.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)\n",
    "\n",
    "# Performance improvement provided by addition of units at hidden layer\n",
    "bla=[]\n",
    "xs = np.arange(2,N+1,1)\n",
    "\n",
    "fig = plt.figure(2,figsize)\n",
    "ax = fig.add_subplot(111)\n",
    "for i in range(N-1):\n",
    "    dif=(best_tanh[i+1,-2]-best_tanh[i,-2])/best_tanh[i,-2]#Relative increase\n",
    "    bla.append(100*dif)\n",
    "    plt.bar(i+2,100*dif, color='gray')\n",
    "    plt.xticks(np.arange(2, 31, 1))\n",
    "\n",
    "e1 = patches.Ellipse(xy=(9,1.15), width=1.4,height=0.45, fill=False)\n",
    "ax.add_patch(e1)\n",
    "e = patches.Ellipse(xy=(12,1.3), width=1.4,height=0.45, fill=False)\n",
    "ax.add_patch(e)\n",
    "\n",
    "ax.set_title('Performance increase by adding units at hidden layer')\n",
    "ax.set_ylabel('Percentage increase at r mean')\n",
    "ax.set_xlabel('Comparisons of unit_(i) and unit_(i-1)')\n",
    "\n",
    "for x,y in zip(xs,bla):\n",
    "    if y < 0:\n",
    "        space = -10\n",
    "    else:\n",
    "        space = 8\n",
    "    label = \"{:.2f}\".format(y)\n",
    "    plt.annotate(label, # this is the text\n",
    "                 (x,y), # this is the point to label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,space), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center\n",
    "fname = 'Percentage increase at r mean-unit-unit' + '.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_test=pd.DataFrame(columns=mlp_test.columns)\n",
    "for i in range(1,N+1):\n",
    "    best_test.loc[i] = mlp_test.loc[idx[i,'tanh','lbfgs','constant']].iloc[0,:]\n",
    "best_test['r2_stdev']=best_test.iloc[:,3:8].std(axis=1)\n",
    "\n",
    "# 4.2 Plot restrictly tanh, lbfgs, constant - Cross validation\n",
    "plt.figure(1,figsize)\n",
    "for i in range(1,N+1):\n",
    "    plt.errorbar(i,best_test.iloc[i-1,-2],yerr=best_test.iloc[i-1,-1],elinewidth=0.5, ecolor='black')#Mean of R2 of the crossvalidation regression\n",
    "    plt.plot(i,best_test.iloc[i-1,-2],'k.')\n",
    "    if i==9:\n",
    "        plt.plot(i,best_test.iloc[i-1,-2],'rx')  \n",
    "plt.title('Mean and standard deviation: r2 mean of test set (actv function: tanh)')\n",
    "plt.ylabel('r2 mean')\n",
    "plt.xlabel('Number of neurons at hidden layer')\n",
    "fname = 'r2 mean of test set x nn' + '.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neurons=9\n",
    "\n",
    "# 4.2 Plot y vs ŷ -> TEST SET\n",
    "plt.figure(1,figsize=(8,8))\n",
    "plt.plot([73,120],[73,120],'-.', color='grey')\n",
    "plt.plot(mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,0],mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,1],'k.')\n",
    "plt.xlabel('Y_test (SO2 real values)')\n",
    "plt.ylabel('Ŷ_test (SO2 estimatives)')\n",
    "plt.ylim((73,120))\n",
    "plt.xlim((73,120))\n",
    "plt.title('MLP regression avaliation: ŷ vs y '+\"\\n (\" +str(neurons)+' units at hidden layer and tanh as activation function)')\n",
    "plt.text(75,115,'R2 test: '+str(round(mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[0,2],4))+'\\n'+'R test: '+str(round(np.sqrt(mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[0,2]),4)), fontsize=12)\n",
    "fname = 'MLP-Regression_y vs ŷ ' +str(neurons)+ '.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)\n",
    "\n",
    "#Histogram -> y-ŷ -> ABSOLUTE VALUES\n",
    "\n",
    "dify=(mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,0]-mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,1])\n",
    "plt.figure(2,figsize=(8,8))\n",
    "plt.hist(sorted(dify),50, density=True, color='gray')\n",
    "plt.title('Histogram of residuals - MLP: '+str(neurons)+' units at hidden layer'+'\\n'+'Absolute values')\n",
    "plt.xlabel('y-ŷ')\n",
    "plt.ylabel('Probability density')\n",
    "plt.text(-14,0.175,'Residuals mean: '+str(round(dify.mean(0),4))+'\\n'+'Residuals standard deviation: '+str(round(dify.std(0),4)), fontsize=11)\n",
    "\n",
    "fname = 'MLP-Histogram of residuals_y-ŷ_absolute-'+str(neurons) + '.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)\n",
    "\n",
    "#Residuals -> ABSOLUTE VALUES\n",
    "plt.figure(3,figsize=(8,8))\n",
    "plt.plot([0,2600],[0,0],'-.', color='grey')\n",
    "plt.plot(dify,'k.')\n",
    "plt.ylabel('y-ŷ')\n",
    "plt.xlabel('Observation')\n",
    "plt.ylim((-13,13))\n",
    "plt.title('Dispersion of the residuals - MLP: '+str(neurons)+' units at hidden layer'+'\\n'+'Absolute values')\n",
    "fname = 'MLP-Dispersion of the residuals MLP_absolute-' +str(neurons)+ '.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)\n",
    "\n",
    "#Histogram -> y-ŷ -> RELATIVE VALUES\n",
    "\n",
    "dify=(mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,0]-mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,1])\n",
    "difyrel=dify/mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,0]\n",
    "plt.figure(4,figsize=(8,8))\n",
    "plt.hist(sorted(difyrel),50, density=True, color='gray')\n",
    "plt.title('Histogram of residuals - MLP: '+str(neurons)+' units at hidden layer'+'\\n'+'Relative values')\n",
    "plt.xlabel('(y-ŷ)/y')\n",
    "plt.ylabel('Probability density')\n",
    "plt.text(-0.15,17.5,'Residuals mean: '+str(round(difyrel.mean(0),4))+'\\n'+'Residuals standard deviation: '+str(round(difyrel.std(0),4)), fontsize=11)\n",
    "\n",
    "fname = 'MLP-Histogram of residuals_y-ŷ_relative-'+str(neurons) + '.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)\n",
    "\n",
    "#Residuals -> RELATIVE VALUES\n",
    "plt.figure(5,figsize=(8,8))\n",
    "plt.plot([0,2600],[0,0],'-.', color='grey')\n",
    "plt.plot(difyrel,'k.')\n",
    "plt.ylabel('(y-ŷ)/y')\n",
    "plt.xlabel('Observation')\n",
    "#plt.ylim((-0.13,0.13))\n",
    "plt.title('Dispersion of the residuals - MLP: '+str(neurons)+' units at hidden layer'+'\\n'+'Relative values')\n",
    "fname = 'MLP-Dispersion of the residuals MLP_relative-' +str(neurons)+ '.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)\n",
    "\n",
    "#Print Results\n",
    "print(\"r2_complete data: \", mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[0,2])\n",
    "print(\"r_complete data: \", np.sqrt(mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[0,2]))\n",
    "print(\" residuals mean and stdev: \", dify.mean(0),dify.std(0))\n",
    "print(\" Maximum and Minimum error: \", abs(dify).max(),abs(dify).min())\n",
    "print(\" Mean absolute error: \",sum(abs(dify))/dify.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MSE: Mean Squared Error\n",
    "mse_mlp=sum((mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,0]-mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,1])*(mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,0]-mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,1]))/mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,0].shape[0]\n",
    "print(mse_mlp)\n",
    "\n",
    "np.mean(mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,0]-mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,1])\n",
    "print(dify.mean(0))\n",
    "print(dify.std(0))\n",
    "\n",
    "print(np.mean((mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,0]-mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,1])/mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,0]),np.mean(dify/mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons=12\n",
    "# 4.2 Plot y vs ŷ -> TEST SET\n",
    "plt.figure(1,figsize=(8,8))\n",
    "plt.plot([73,120],[73,120],'-.', color='grey')\n",
    "plt.plot(mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,0],mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,1],'k.')\n",
    "plt.xlabel('Y_test (SO2 real values)')\n",
    "plt.ylabel('Ŷ_test (SO2 estimatives)')\n",
    "plt.ylim((73,120))\n",
    "plt.xlim((73,120))\n",
    "plt.title('MLP regression avaliation: ŷ vs y '+\"\\n (\" +str(neurons)+' units at hidden layer and tanh as activation function)')\n",
    "plt.text(75,115,'R2 test: '+str(round(mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[0,2],4))+'\\n'+'R test: '+str(round(np.sqrt(mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[0,2]),4)), fontsize=12)\n",
    "fname = 'MLP-Regression_y vs ŷ ' +str(neurons)+ '.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)\n",
    "\n",
    "#Histogram -> y-ŷ -> ABSOLUTE VALUES\n",
    "\n",
    "dify=(mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,0]-mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,1])\n",
    "plt.figure(2,figsize=(8,8))\n",
    "plt.hist(sorted(dify),50, density=True, color='gray')\n",
    "plt.title('Histogram of residuals - MLP: '+str(neurons)+' units at hidden layer'+'\\n'+'Absolute values')\n",
    "plt.xlabel('y-ŷ')\n",
    "plt.ylabel('Probability density')\n",
    "plt.text(-12,0.155,'Residuals mean: '+str(round(dify.mean(0),4))+'\\n'+'Residuals standard deviation: '+str(round(dify.std(0),4)), fontsize=11)\n",
    "\n",
    "fname = 'MLP-Histogram of residuals_y-ŷ_absolute-'+str(neurons) + '.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)\n",
    "\n",
    "#Residuals -> ABSOLUTE VALUES\n",
    "plt.figure(3,figsize=(8,8))\n",
    "plt.plot([0,2600],[0,0],'-.', color='grey')\n",
    "plt.plot(dify,'k.')\n",
    "plt.ylabel('y-ŷ')\n",
    "plt.xlabel('Observation')\n",
    "plt.ylim((-13,13))\n",
    "plt.title('Dispersion of the residuals - MLP: '+str(neurons)+' units at hidden layer'+'\\n'+'Absolute values')\n",
    "fname = 'MLP-Dispersion of the residuals MLP_absolute-' +str(neurons)+ '.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)\n",
    "\n",
    "#Histogram -> y-ŷ -> RELATIVE VALUES\n",
    "\n",
    "dify=(mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,0]-mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,1])\n",
    "difyrel=dify/mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,0]\n",
    "plt.figure(4,figsize=(8,8))\n",
    "plt.hist(sorted(difyrel),50, density=True, color='gray')\n",
    "plt.title('Histogram of residuals - MLP: '+str(neurons)+' units at hidden layer'+'\\n'+'Relative values')\n",
    "plt.xlabel('(y-ŷ)/y')\n",
    "plt.ylabel('Probability density')\n",
    "plt.text(-0.15,16,'Residuals mean: '+str(round(difyrel.mean(0),4))+'\\n'+'Residuals standard deviation: '+str(round(difyrel.std(0),4)), fontsize=11)\n",
    "\n",
    "fname = 'MLP-Histogram of residuals_y-ŷ_relative-'+str(neurons) + '.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)\n",
    "\n",
    "#Residuals -> RELATIVE VALUES\n",
    "plt.figure(5,figsize=(8,8))\n",
    "plt.plot([0,2600],[0,0],'-.', color='grey')\n",
    "plt.plot(difyrel,'k.')\n",
    "plt.ylabel('(y-ŷ)/y')\n",
    "plt.xlabel('Observation')\n",
    "#plt.ylim((-0.13,0.13))\n",
    "plt.title('Dispersion of the residuals - MLP: '+str(neurons)+' units at hidden layer'+'\\n'+'Relative values')\n",
    "fname = 'MLP-Dispersion of the residuals MLP_relative-' +str(neurons)+ '.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE: Mean Squared Error\n",
    "#neurons = 12\n",
    "mse_mlp=sum((mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,0]-mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,1])*(mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,0]-mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,1]))/mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,0].shape[0]\n",
    "print(mse_mlp)\n",
    "\n",
    "np.mean(mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,0]-mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,1])\n",
    "print(dify.mean(0))\n",
    "print(dify.std(0))\n",
    "\n",
    "print(np.mean((mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,0]-mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,1])/mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,0]),np.mean(dify/mlp_test.loc[idx[neurons,'tanh','lbfgs','constant']].iloc[:,0]))\n",
    "\n",
    "print(work.columns)\n",
    "print(work.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Variables Identification\n",
    "listvar=data.columns[:-1]\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTIPLE LINEAR REGRESSION \n",
    "reg=sk.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)\n",
    "#Treinar o modelo com todos os dados\n",
    "freg=reg.fit(xtrain,ytrain)\n",
    "parr=freg.get_params(deep=True)\n",
    "#Prediction\n",
    "yhat=freg.predict(xtest) #Test Prediction\n",
    "Yh=scalery.inverse_transform(yhat.reshape(-1,1)) #Return to original scale\n",
    "R2test=freg.score(xtest,ytest) #Real test set\n",
    "Yhat=np.reshape(Yh,-1)\n",
    "#Mean Squared Error - Prediction\n",
    "mse_mlr=sum((yte-Yhat)*(yte-Yhat))/yte.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICTION TRAINING SET\n",
    "ytrhat=freg.predict(xtrain) #Test Prediction\n",
    "ytrh=scalery.inverse_transform(ytrhat.reshape(-1,1)) #Return to original scale\n",
    "R2tr=freg.score(xtrain,ytrain) #Real test set\n",
    "Ytrhat=np.reshape(ytrh,-1)\n",
    "#Mean Squared Error - Prediction\n",
    "mse_mlrtrain=sum((ytr-Ytrhat)*(ytr-Ytrhat))/ytr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ---------  SENSITIVITY ANALYSIS  -------- ###\n",
    "yMLR = {'Y':np.array(ytr),'yhat':Ytrhat}\n",
    "#Coefficient of Determination\n",
    "R2MLR={'r2train':R2tr}\n",
    "#Mean Squared Error (MSE)\n",
    "MSE_mlr={'MSEtrain':mse_mlrtrain}\n",
    "#sum all deviations\n",
    "sumdevR2MLR=0\n",
    "#Mean of each variable\n",
    "mean=xtrain.mean(axis=0)\n",
    "stdev=xtrain.std(axis=0)\n",
    "#Modifications in the input data: substitution of each variable for the its mean value\n",
    "for i in range(len(listvar)):# -1 -> due to output    \n",
    "    x1=xtrain.copy()\n",
    "    stdev=xtrain.std(axis=0)\n",
    "    \n",
    "    #mst='m' #Disturbancy: Only mean\n",
    "    x1[:250,i]=mean[i]+0.5*stdev[i]\n",
    "    x1[250:500,i]=mean[i]+1*stdev[i]\n",
    "    x1[500:750,i]=mean[i]+1.5*stdev[i]\n",
    "    x1[750:,i]=mean[i]+2*stdev[i]\n",
    "    mst='m+kstd' #Disturbancy: Mean+-k*stdev\n",
    "    \n",
    "    x1[:,i]=mean[i]\n",
    "    #Prediction\n",
    "    y1=freg.predict(x1)\n",
    "    yh1=scalery.inverse_transform(y1.reshape(-1,1)) #Return to original scale\n",
    "    Y1=np.reshape(yh1,-1)\n",
    "    r21=freg.score(x1,ytrain) \n",
    "    #Mean Squared Error - Prediction\n",
    "    mse_mlr1=sum((ytr-Y1)*(ytr-Y1))/ytr.shape[0]\n",
    "    #Register\n",
    "    yMLR.update({'yhat_mean_'+str(listvar[i]):Y1})\n",
    "    R2MLR.update({'R2_mean_'+str(listvar[i]):r21})\n",
    "    MSE_mlr.update({'MSE_mean_'+str(listvar[i]):mse_mlr1})\n",
    "    sumdevR2MLR+=(R2tr-r21)\n",
    "yMLR\n",
    "MSE_mlr\n",
    "#R2MLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSEmlr=pd.DataFrame.from_dict(MSE_mlr,orient='index', columns=['MSE_MLR'])\n",
    "MSEmlr.sort_values('MSE_MLR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MEAN SQUARED ERROR CHART - MLR\n",
    "MSEmlr=pd.DataFrame.from_dict(MSE_mlr,orient='index', columns=['MSE_MLR'])\n",
    "msemlr = MSEmlr.sort_values('MSE_MLR',ascending=False)\n",
    "plt.figure()\n",
    "ms=msemlr.plot.bar(figsize=(10,7), title='MSE MLR',color='gray')\n",
    "ms.set_ylabel('MSE')\n",
    "for p in ms.patches:\n",
    "    ms.annotate(str(round(p.get_height(),2)), (p.get_x() *0.995, p.get_height() * 1.005))\n",
    "\n",
    "fname = 'MLR-Sensitivity.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural network Model\n",
    "## 4.2 Multilayer Perceptron\n",
    "#Number of hidden layers variation\n",
    "N=30 #maximal number of neurons at hidden layer\n",
    "kn=5 #number of splits at cross-validation procedure\n",
    "t=1e-3 #Tolerance (modifications at r2 - > training the net)\n",
    "niter=7\n",
    "ns=9\n",
    "\n",
    "nn=MLPRegressor(hidden_layer_sizes=(ns+1,),\n",
    "                           activation='tanh', \n",
    "                           solver='lbfgs',\n",
    "                           batch_size='auto',\n",
    "                           learning_rate= 'constant',\n",
    "                           max_iter=1000,\n",
    "                           random_state=rs,\n",
    "                           tol=t,\n",
    "                           n_iter_no_change=niter,\n",
    "                           verbose=False)\n",
    "#Training with all data\n",
    "rb=nn.fit(xtrain,ytrain)\n",
    "#Test with the extra testing set - For each cross validation\n",
    "yhat=nn.predict(xtest)\n",
    "yh=yhat.reshape(-1,1)\n",
    "R2=rb.score(xtest,ytest)\n",
    "Yh=scalery.inverse_transform(yh)\n",
    "Yhat=np.reshape(Yh,-1)\n",
    "#Mean Squared Error - Prediction\n",
    "mse_mlp=sum((yte-Yhat)*(yte-Yhat))/yte.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICTION TRAINING SET\n",
    "ytrhat=nn.predict(xtrain) #Test Prediction\n",
    "ytrh=scalery.inverse_transform(ytrhat.reshape(-1,1)) #Return to original scale\n",
    "R2tr=nn.score(xtrain,ytrain) #Real test set\n",
    "Ytrhat=np.reshape(ytrh,-1)\n",
    "#Mean Squared Error - Prediction\n",
    "mse_mlptrain=sum((ytr-Ytrhat)*(ytr-Ytrhat))/ytr.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ---------  SENSITIVITY ANALYSIS  -------- ###\n",
    "yMLP = {'Y':np.array(ytr),'yhat':Ytrhat}\n",
    "#Coefficient of Determination\n",
    "R2MLP={'Training':R2tr}\n",
    "#Mean Squared Error (MSE)\n",
    "MSE_mlp={'Training':mse_mlptrain}\n",
    "#sum all deviations\n",
    "sumdevR2MLP=0\n",
    "#Mean of each variable - original range\n",
    "meano=xtr.mean(axis=0)\n",
    "stdevo=xtr.std(axis=0)\n",
    "#Mean of each variable - scaled\n",
    "mean=xtrain.mean(axis=0)\n",
    "stdev=xtrain.std(axis=0)\n",
    "\n",
    "#Modifications in the input data: substitution of each variable for the its mean value\n",
    "for i in range(len(listvar)):# -1 -> due to output    \n",
    "    x1=xtrain.copy()\n",
    "    x1o=xtr.copy()\n",
    "    #x1[:,i]=mean[i]\n",
    "\n",
    "    #Disturbancy: Only mean - original range\n",
    "    x1o.iloc[:250,i]=meano[i]+0.5*stdevo[i]\n",
    "    x1o.iloc[250:500,i]=meano[i]+1*stdevo[i]\n",
    "    x1o.iloc[500:750,i]=meano[i]+1.5*stdevo[i]\n",
    "    x1o.iloc[750:,i]=meano[i]+2*stdevo[i]\n",
    "\n",
    "    #mst='m' #Disturbancy: Only mean - Scaled\n",
    "    x1[:250,i]=mean[i]+0.5*stdev[i]\n",
    "    x1[250:500,i]=mean[i]+1*stdev[i]\n",
    "    x1[500:750,i]=mean[i]+1.5*stdev[i]\n",
    "    x1[750:,i]=mean[i]+2*stdev[i]\n",
    "    mst='m+kstd' #Disturbancy: Mean+-k*stdev\n",
    "\n",
    "    #Prediction\n",
    "    y1=nn.predict(x1)\n",
    "    yh1=scalery.inverse_transform(y1.reshape(-1,1)) #Return to original scale\n",
    "    Y1=np.reshape(yh1,-1)\n",
    "    r21=nn.score(x1,ytrain) \n",
    "    #Mean Squared Error - Prediction\n",
    "    mse_mlp1=sum((ytr-Y1)*(ytr-Y1))/ytr.shape[0]\n",
    "    #Register\n",
    "    yMLP.update({'yhat_mean_'+str(listvar[i]):Y1})\n",
    "    R2MLP.update({str(listvar[i]):r21})\n",
    "    MSE_mlp.update({str(listvar[i]):mse_mlp1})\n",
    "    sumdevR2MLP+=(R2tr-r21)\n",
    "\n",
    "    #Plot Disturbances\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.plot(x1o.reset_index().iloc[:,i+1],'k--',label='Disturbed')\n",
    "    plt.plot(xtr.reset_index().iloc[:,i+1],'k',alpha=0.5,label='Not Disturbed')\n",
    "    plt.legend()\n",
    "    plt.title(x1o.columns[i])\n",
    "    plt.ylabel(x1o.columns[i])\n",
    "    fname = 'Disturbance'+str(x1o.columns[i][:-4])+'.png'\n",
    "    plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)\n",
    "\n",
    "yMLP\n",
    "MSE_mlp\n",
    "#R2MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1o.columns[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform in DataFrame\n",
    "MSEmlp=pd.DataFrame.from_dict(MSE_mlp,orient='index', columns=['MSE_MLP'])\n",
    "R2mlp=pd.DataFrame.from_dict(R2MLP,orient='index', columns=['R2_MLP'])\n",
    "Ymlp=pd.DataFrame.from_dict(yMLP)\n",
    "MLP_results=MSEmlp.copy()\n",
    "MLP_results['R2_MLP']=R2mlp.copy()\n",
    "print(MLP_results)\n",
    "\n",
    "'''\n",
    "fname = '\\Sensitivity_MLP.xlsx'\n",
    "writer = pd.ExcelWriter(dir+fname, engine='openpyxl')\n",
    "\n",
    "Ymlp.to_excel(writer,sheet_name='ymlp')\n",
    "MLP_results.to_excel(writer,sheet_name='MSEr2')\n",
    "\n",
    "        \n",
    "writer.save()\n",
    "writer.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MEAN SQUARED ERROR CHART\n",
    "MSEmlp=pd.DataFrame.from_dict(MSE_mlp,orient='index', columns=['MSE_MLP'])\n",
    "msemlp = MSEmlp.sort_values('MSE_MLP',ascending=False)\n",
    "plt.figure()\n",
    "mse=msemlp.plot.bar(figsize=(10,7), title='MSE MLP', color='gray')\n",
    "plt.plot((0,14),(MSEmlp.iloc[0],MSEmlp.iloc[0]),'k--')\n",
    "mse.set_ylabel('MSE')\n",
    "for p in mse.patches:\n",
    "    mse.annotate(str(round(p.get_height(),2)), (p.get_x() * 0.995, p.get_height() * 1.005))\n",
    "    \n",
    "fname = 'MLP-Sensitivity.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ver\n",
    "#https://www.python-course.eu/graphs_python.php\n",
    "print(MSEmlp.iloc[1::,:]/MSEmlp.iloc[1::,:].min())\n",
    "print(MSEmlp.iloc[1::,:]/MSEmlp.iloc[1::,:].sum()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MEAN SQUARED ERROR CHART\n",
    "MSEmlp=pd.DataFrame.from_dict(MSE_mlp,orient='index', columns=['MSE'])\n",
    "#msemlp = MSEmlp.sort_values('MSE_MLP',ascending=False)\n",
    "plt.figure()\n",
    "mse=MSEmlp.plot.bar(figsize=(10,7), title='MSE MLP', color='gray',alpha=0.7)\n",
    "plt.plot((0,15),(MSEmlp.iloc[0],MSEmlp.iloc[0]),'k--',alpha=0.7)\n",
    "mse.set_ylabel('Erro quadrático médio (MSE)')\n",
    "for p in mse.patches:\n",
    "    mse.annotate(str(round(p.get_height(),3)), (p.get_x() * 0.995, p.get_height() * 1.005))\n",
    "plt.annotate('MSE referência = '+str(round(MSEmlp.iloc[0][0],3))+'ppm',(7,0.5*MSEmlp.iloc[0]))\n",
    "    \n",
    "fname = 'MLP-Sensitivity-desorder.png'\n",
    "plt.savefig(os.path.join(dir, fname), bbox_inches='tight', format='png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VAriables that most inffluence\n",
    "varinf=['13-F_TA_T/H', '7-F_PA_T/H', '1-F_FUEL_T/H','10-F_SA_T/H']\n",
    "ooo=['18-C_SO2_PPM']\n",
    "for i in range(len(varinf)):\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.plot(work[varinf[i]],work[ooo],'k.')\n",
    "    plt.ylabel(ooo[0])\n",
    "    plt.xlabel(varinf[i])\n",
    "    '''\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.plot(work[ooo],'k',label=ooo[0])\n",
    "    plt.plot(work[varinf[i]],'b',label=varinf[i])\n",
    "    plt.ylabel('Values')\n",
    "    plt.xlabel('Time serie')\n",
    "    plt.legend()\n",
    "    '''\n",
    "    fig = plt.figure(figsize=(10,7))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(work[ooo],'k',alpha=0.6,label=ooo[0])\n",
    "    ax2.plot(work[varinf[i]],'k-.',alpha=1,label=varinf[i])\n",
    "\n",
    "    ax1.set_xlabel('X data')\n",
    "    ax1.set_ylabel(str(ooo[0])+' data', color='k')\n",
    "    ax2.set_ylabel(str(varinf[i])+' data', color='k')\n",
    "    ax2.set_ylim(work[varinf[i]].min()*0.9,work[varinf[i]].max()*1.01)\n",
    "    fig.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste=work[varinf]\n",
    "teste[ooo]=work[ooo]\n",
    "\n",
    "#CORRELATION MATRIX\n",
    "corw=teste.corr() #Correlation Matrix\n",
    "corw.style.background_gradient(cmap='PuBu')#'coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
